{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c175d392",
   "metadata": {},
   "source": [
    "# Titulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ae827",
   "metadata": {},
   "source": [
    "texto de introducao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805146bf",
   "metadata": {},
   "source": [
    "### Go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d3038",
   "metadata": {},
   "source": [
    "texto introducao do Go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb10c8",
   "metadata": {},
   "source": [
    "### Implementação do Go Graficamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95708b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Select board size (7 or 9): \n",
      "7\n",
      "Select Player 1 (H for human, A for AI): \n",
      "1\n",
      "Select Player 2 (H for human, A for AI): \n",
      "1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No file 'wav/zoink.wav' found in working directory 'C:\\Users\\beatr\\Desktop\\UNI\\3ºANO\\Lab_IA_CD\\Projeto2 - Attax&Go\\Trabalho'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 345>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    352\u001b[0m player2 \u001b[38;5;241m=\u001b[39m HumanPlayer() \u001b[38;5;28;01mif\u001b[39;00m player2_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m AI(color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    354\u001b[0m g \u001b[38;5;241m=\u001b[39m Game(size, player1, player2)\n\u001b[1;32m--> 356\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_pygame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m g\u001b[38;5;241m.\u001b[39mclear_screen()\n\u001b[0;32m    358\u001b[0m g\u001b[38;5;241m.\u001b[39mdraw()\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mGame.init_pygame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m screen \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((BOARD_WIDTH, BOARD_WIDTH))\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;241m=\u001b[39m screen\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZOINK \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwav/zoink.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLICK \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mSound(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav/click.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mSysFont(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marial\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No file 'wav/zoink.wav' found in working directory 'C:\\Users\\beatr\\Desktop\\UNI\\3ºANO\\Lab_IA_CD\\Projeto2 - Attax&Go\\Trabalho'."
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import itertools\n",
    "import sys\n",
    "import networkx as nx\n",
    "import collections\n",
    "from pygame import gfxdraw\n",
    "\n",
    "# Define board size\n",
    "print(\"Select board size (7 or 9): \")\n",
    "size = int(input())\n",
    "\n",
    "# Game constants\n",
    "BOARD_BROWN = (141, 104, 75)  # Change color as desired\n",
    "BOARD_WIDTH = 800  # New size of the board\n",
    "BOARD_BORDER = 75\n",
    "STONE_RADIUS = int(abs(BOARD_WIDTH / size * 20 * 0.02))  # Adjust stone size to grid size\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "TURN_POS = (BOARD_BORDER, 20)\n",
    "SCORE_POS = (BOARD_BORDER, BOARD_WIDTH - BOARD_BORDER + 30)\n",
    "DOT_RADIUS = 2\n",
    "\n",
    "\n",
    "def make_grid(size):\n",
    "    # Return list of (start_point, end_point pairs) defining gridlines\n",
    "    start_points, end_points = [], []\n",
    "\n",
    "    # Vertical start points (constant y)\n",
    "    xs = np.linspace(BOARD_BORDER, BOARD_WIDTH - BOARD_BORDER, size)\n",
    "    ys = np.full((size), BOARD_BORDER)\n",
    "    start_points += list(zip(xs, ys))\n",
    "\n",
    "    # Horizontal start points (constant x)\n",
    "    xs = np.full((size), BOARD_BORDER)\n",
    "    ys = np.linspace(BOARD_BORDER, BOARD_WIDTH - BOARD_BORDER, size)\n",
    "    start_points += list(zip(xs, ys))\n",
    "\n",
    "    # Vertical end points (constant y)\n",
    "    xs = np.linspace(BOARD_BORDER, BOARD_WIDTH - BOARD_BORDER, size)\n",
    "    ys = np.full((size), BOARD_WIDTH - BOARD_BORDER)\n",
    "    end_points += list(zip(xs, ys))\n",
    "\n",
    "    # Horizontal end points (constant x)\n",
    "    xs = np.full((size), BOARD_WIDTH - BOARD_BORDER)\n",
    "    ys = np.linspace(BOARD_BORDER, BOARD_WIDTH - BOARD_BORDER, size)\n",
    "    end_points += list(zip(xs, ys))\n",
    "\n",
    "    return start_points, end_points\n",
    "\n",
    "\n",
    "class HumanPlayer:\n",
    "    def make_move(self, board):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AI:\n",
    "    def __init__(self, color):\n",
    "        self.color = color\n",
    "\n",
    "    def make_move(self, board):\n",
    "        # Implement AI move generation logic here\n",
    "        # Construct alphaZero on the side and import here\n",
    "\n",
    "        # random move generator (valid moves only)\n",
    "        valid_moves = [(col, row) for col in range(board.shape[0]) for row in range(board.shape[1]) if board[col, row] == 0]\n",
    "        if not valid_moves:\n",
    "            return None  # No valid moves available\n",
    "\n",
    "        return valid_moves[np.random.choice(len(valid_moves))]\n",
    "\n",
    "\n",
    "def xy_to_colrow(x, y, size):\n",
    "    inc = (BOARD_WIDTH - 2 * BOARD_BORDER) / (size - 1)\n",
    "    x_dist = x - BOARD_BORDER\n",
    "    y_dist = y - BOARD_BORDER\n",
    "    col = int(round(x_dist / inc))\n",
    "    row = int(round(y_dist / inc))\n",
    "    return col, row\n",
    "\n",
    "\n",
    "def colrow_to_xy(col, row, size):\n",
    "    inc = (BOARD_WIDTH - 2 * BOARD_BORDER) / (size - 1)\n",
    "    x = int(BOARD_BORDER + col * inc)\n",
    "    y = int(BOARD_BORDER + row * inc)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def has_no_liberties(board, group):\n",
    "    for x, y in group:\n",
    "        if x > 0 and board[x - 1, y] == 0:\n",
    "            return False\n",
    "        if y > 0 and board[x, y - 1] == 0:\n",
    "            return False\n",
    "        if x < board.shape[0] - 1 and board[x + 1, y] == 0:\n",
    "            return False\n",
    "        if y < board.shape[0] - 1 and board[x, y + 1] == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_stone_groups(board, color):\n",
    "    size = board.shape[0]\n",
    "    color_code = 1 if color == \"black\" else 2\n",
    "    xs, ys = np.where(board == color_code)\n",
    "    graph = nx.grid_graph(dim=[size, size])\n",
    "    stones = set(zip(xs, ys))\n",
    "    all_spaces = set(itertools.product(range(size), range(size)))\n",
    "    stones_to_remove = all_spaces - stones\n",
    "    graph.remove_nodes_from(stones_to_remove)\n",
    "    return nx.connected_components(graph)\n",
    "\n",
    "\n",
    "def is_valid_move(col, row, board):\n",
    "    if col < 0 or col >= board.shape[0]:\n",
    "        return False\n",
    "    if row < 0 or row >= board.shape[0]:\n",
    "        return False\n",
    "    return board[col, row] == 0\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, size, player1, player2):\n",
    "        self.board = np.zeros((size, size))\n",
    "        self.size = size\n",
    "        self.black_turn = True\n",
    "        self.prisoners = collections.defaultdict(int)\n",
    "        self.start_points, self.end_points = make_grid(self.size)\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.player1_passed = False\n",
    "        self.player2_passed = False\n",
    "        self.current_player = self.player1 if self.black_turn else self.player2\n",
    "\n",
    "    def init_pygame(self):\n",
    "        pygame.init()\n",
    "        screen = pygame.display.set_mode((BOARD_WIDTH, BOARD_WIDTH))\n",
    "        self.screen = screen\n",
    "        self.ZOINK = pygame.mixer.Sound(\"wav/zoink.wav\")\n",
    "        self.CLICK = pygame.mixer.Sound(\"wav/click.wav\")\n",
    "        self.font = pygame.font.SysFont(\"arial\", 30)\n",
    "\n",
    "    def clear_screen(self):\n",
    "        # Fill board and add gridlines\n",
    "        self.screen.fill(BOARD_BROWN)\n",
    "        for start_point, end_point in zip(self.start_points, self.end_points):\n",
    "            pygame.draw.line(self.screen, BLACK, start_point, end_point)\n",
    "\n",
    "        # Add guide dots\n",
    "        guide_dots = [3, self.size // 2, self.size - 4]\n",
    "        for col, row in itertools.product(guide_dots, guide_dots):\n",
    "            x, y = colrow_to_xy(col, row, self.size)\n",
    "            gfxdraw.aacircle(self.screen, x, y, DOT_RADIUS, BLACK)\n",
    "            gfxdraw.filled_circle(self.screen, x, y, DOT_RADIUS, BLACK)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def check_territory(self, col, row, color):\n",
    "        visited = set()\n",
    "\n",
    "        def dfs(x, y):\n",
    "            if x < 0 or x >= self.size or y < 0 or y >= self.size:\n",
    "                return False\n",
    "            if self.board[x, y] == color or (x, y) in visited:\n",
    "                return True\n",
    "            visited.add((x, y))\n",
    "            return all(dfs(nx, ny) for nx, ny in ((x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)))\n",
    "\n",
    "        return dfs(col, row)\n",
    "\n",
    "    def calculate_score(self):\n",
    "        \"\"\"Calculate the score of the game.\"\"\"\n",
    "        territory_black, territory_white, stones_black, stones_white = 0, 0, 0, 0\n",
    "\n",
    "        for col in range(self.size):\n",
    "            for row in range(self.size):\n",
    "                color = self.board[col, row]\n",
    "                if color == 1:\n",
    "                    stones_black += 1\n",
    "                elif color == 2:\n",
    "                    stones_white += 1\n",
    "                elif color == 0:\n",
    "                    neighbors = [(col + i, row + j) for i, j in [(-1, 0), (1, 0), (0, -1), (0, 1)] if\n",
    "                                0 <= col + i < self.size and 0 <= row + j < self.size]\n",
    "                    if all(0 <= n_col < self.size and 0 <= n_row < self.size and self.board[n_col, n_row] != color\n",
    "                        for n_col, n_row in neighbors):\n",
    "                        # Empty intersection surrounded by opponent's stones\n",
    "                        territory_black += 1\n",
    "                    elif all(0 <= n_col < self.size and 0 <= n_row < self.size and self.board[n_col, n_row] != color\n",
    "                            for n_col, n_row in neighbors):\n",
    "                        # Empty intersection surrounded by opponent's stones\n",
    "                        territory_white += 1\n",
    "\n",
    "        return territory_black, territory_white, stones_black, stones_white\n",
    "\n",
    "    def draw(self):\n",
    "        # Draw stones - filled circle and antialiased ring\n",
    "        self.clear_screen()\n",
    "        for col, row in zip(*np.where(self.board == 1)):\n",
    "            x, y = colrow_to_xy(col, row, self.size)\n",
    "            gfxdraw.aacircle(self.screen, x, y, STONE_RADIUS, BLACK)\n",
    "            gfxdraw.filled_circle(self.screen, x, y, STONE_RADIUS, BLACK)\n",
    "        for col, row in zip(*np.where(self.board == 2)):\n",
    "            x, y = colrow_to_xy(col, row, self.size)\n",
    "            gfxdraw.aacircle(self.screen, x, y, STONE_RADIUS, WHITE)\n",
    "            gfxdraw.filled_circle(self.screen, x, y, STONE_RADIUS, WHITE)\n",
    "\n",
    "        # Text for score and turn info\n",
    "        territory_black, territory_white, stones_black, stones_white = self.calculate_score()\n",
    "        score_msg = (\n",
    "            f\"Black - Territory: {territory_black}, Stones: {stones_black} | White - Territory: {territory_white}, Stones: {stones_white}\"\n",
    "        )\n",
    "        txt = self.font.render(score_msg, True, BLACK)\n",
    "        self.screen.blit(txt, SCORE_POS)\n",
    "        turn_msg = (\n",
    "            f\"{'Black' if self.black_turn else 'White'} to move. \"\n",
    "            + \"Click to place a stone, press P to pass.\"\n",
    "        )\n",
    "        txt = self.font.render(turn_msg, True, BLACK)\n",
    "        self.screen.blit(txt, TURN_POS)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def update(self):\n",
    "        events = pygame.event.get()\n",
    "        for event in events:\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.print_final_scores()\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        if not self.current_player_is_human():\n",
    "            self.make_ai_move()\n",
    "    def make_ai_move(self):\n",
    "        ai_col, ai_row = self.current_player.make_move(self.board)\n",
    "        while not is_valid_move(ai_col, ai_row, self.board):\n",
    "            ai_col, ai_row = self.current_player.make_move(self.board)\n",
    "        self.board[ai_col, ai_row] = 2 if self.current_player == self.player2 else 1\n",
    "        self.black_turn = not self.black_turn\n",
    "        self.current_player = self.player1 if self.black_turn else self.player2  # Update current player\n",
    "        self.draw()\n",
    "\n",
    "        if self.check_end_game():\n",
    "            self.print_final_scores()\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "    def handle_click(self):\n",
    "        # get board position\n",
    "        x, y = pygame.mouse.get_pos()\n",
    "        col, row = xy_to_colrow(x, y, self.size)\n",
    "        if not is_valid_move(col, row, self.board):\n",
    "            self.ZOINK.play()\n",
    "            return\n",
    "\n",
    "        # update board array\n",
    "        self.board[col, row] = 1 if self.current_player == self.player1 else 2\n",
    "\n",
    "        # get stone groups for black and white\n",
    "        self_color = \"black\" if self.black_turn else \"white\"\n",
    "        other_color = \"white\" if self.black_turn else \"black\"\n",
    "\n",
    "        # handle captures\n",
    "        prisoners_captured = 0\n",
    "        for group in list(get_stone_groups(self.board, other_color)):\n",
    "            if has_no_liberties(self.board, group):\n",
    "                prisoners_captured += len(group)\n",
    "                for i, j in group:\n",
    "                    self.board[i, j] = 0\n",
    "\n",
    "        # update prisoners count\n",
    "        if self_color == \"black\":\n",
    "            self.prisoners['white'] += prisoners_captured\n",
    "        else:\n",
    "            self.prisoners['black'] += prisoners_captured\n",
    "\n",
    "        # change turns and draw screen\n",
    "        self.CLICK.play()\n",
    "        self.black_turn = not self.black_turn\n",
    "        self.current_player = self.player1 if self.black_turn else self.player2  # Update current player\n",
    "        self.draw()\n",
    "\n",
    "        # If it's the AI's turn, let AI make a move\n",
    "        if not self.current_player_is_human():\n",
    "            ai_col, ai_row = self.current_player.make_move(self.board)\n",
    "            while not is_valid_move(ai_col, ai_row, self.board):\n",
    "                ai_col, ai_row = self.current_player.make_move(self.board)\n",
    "            self.board[ai_col, ai_row] = 2 if self.current_player == self.player2 else 1\n",
    "            self.black_turn = not self.black_turn\n",
    "            self.current_player = self.player1 if self.black_turn else self.player2  # Update current player\n",
    "            self.draw()\n",
    "\n",
    "    def pass_move(self):\n",
    "        if self.black_turn:\n",
    "            self.player1_passed = True\n",
    "        else:\n",
    "            self.player2_passed = True\n",
    "\n",
    "        self.black_turn = not self.black_turn\n",
    "        self.draw()\n",
    "\n",
    "        if self.check_end_game():\n",
    "            self.print_final_scores()\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "\n",
    "    def check_end_game(self):\n",
    "        if self.passed_twice() or not any(\n",
    "            is_valid_move(col, row, self.board) for col in range(self.size) for row in range(self.size)\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def passed_twice(self):\n",
    "        return self.player1_passed and self.player2_passed\n",
    "\n",
    "    def print_final_scores(self):\n",
    "        territory_black, territory_white, stones_black, stones_white = self.calculate_score()\n",
    "        print(\"Game Over!\")\n",
    "        print(f\"Black - Territory: {territory_black}, Stones: {stones_black}; Total Score: {territory_black+stones_black}\")\n",
    "        print(f\"White - Territory: {territory_white}, Stones: {stones_white}; Total Score: {territory_white+stones_white}\")\n",
    "\n",
    "        if territory_black + stones_black > territory_white + stones_white:\n",
    "            print(\"Black wins!\")\n",
    "        elif territory_black + stones_black < territory_white + stones_white:\n",
    "            print(\"White wins!\")\n",
    "        else:\n",
    "            print(\"It's a tie!\")\n",
    "\n",
    "    def update(self):\n",
    "        events = pygame.event.get()\n",
    "        for event in events:\n",
    "            if event.type == pygame.MOUSEBUTTONUP:\n",
    "                self.handle_click()\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.print_final_scores()\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "            if event.type == pygame.KEYUP:\n",
    "                if event.key == pygame.K_p:\n",
    "                    self.pass_move()\n",
    "\n",
    "    def current_player_is_human(self):\n",
    "        return self.current_player is None or isinstance(self.current_player, HumanPlayer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Select Player 1 (H for human, A for AI): \")\n",
    "    player1_type = input().upper()\n",
    "    player1 = HumanPlayer() if player1_type == \"H\" else AI(color=\"black\")\n",
    "\n",
    "    print(\"Select Player 2 (H for human, A for AI): \")\n",
    "    player2_type = input().upper()\n",
    "    player2 = HumanPlayer() if player2_type == \"H\" else AI(color=\"white\")\n",
    "\n",
    "    g = Game(size, player1, player2)\n",
    "\n",
    "    g.init_pygame()\n",
    "    g.clear_screen()\n",
    "    g.draw()\n",
    "\n",
    "    while True:\n",
    "        g.update()\n",
    "\n",
    "        if not g.current_player_is_human():\n",
    "            g.make_ai_move()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069420b",
   "metadata": {},
   "source": [
    "### Classe Go() - Lógica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc25a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logica para usar no AlphaZero\n",
    "\n",
    "class Go():\n",
    "\n",
    "    EMPTY = 0\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    BLACKMARKER = 4\n",
    "    WHITEMARKER = 5\n",
    "    LIBERTY = 8\n",
    "\n",
    "    def __init__(self, size, komi):\n",
    "        self.row_count = size\n",
    "        self.column_count = size\n",
    "        self.komi = 5.5\n",
    "        self.action_size = self.row_count * self.column_count + 1\n",
    "        self.liberties = []\n",
    "        self.block = []\n",
    "        self.seki_liberties = []\n",
    "        \n",
    "        def get_initial_state(self):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a board of the argument size filled of zeros.\n",
    "\n",
    "        # Retuns:\n",
    "        Empty board full of zeros\n",
    "        '''\n",
    "        board = np.zeros((self.row_count, self.column_count))\n",
    "        return board\n",
    "    \n",
    "    \n",
    "    def count(self, x, y, state: list, player:int , liberties: list, block: list) -> tuple[list, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Counts the number of liberties of a stone and the number of stones in a block.\n",
    "        Follows a recursive approach to count the liberties of a stone and the number of stones in a block.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing the number of liberties and the number of stones in a block.\n",
    "        '''\n",
    "        \n",
    "        #initialize piece\n",
    "        piece = state[y][x]\n",
    "        #if there's a stone at square of the given player\n",
    "        if piece == player:\n",
    "            #save stone coords\n",
    "            block.append((y,x))\n",
    "            #mark the stone\n",
    "            if player == self.BLACK:\n",
    "                state[y][x] = self.BLACKMARKER\n",
    "            else:\n",
    "                state[y][x] = self.WHITEMARKER\n",
    "            \n",
    "            #look for neighbours recursively\n",
    "            if y-1 >= 0:\n",
    "                liberties, block = self.count(x,y-1,state,player,liberties, block) #walk north\n",
    "            if x+1 < self.column_count:\n",
    "                liberties, block = self.count(x+1,y,state,player,liberties, block) #walk east\n",
    "            if y+1 < self.row_count:\n",
    "                liberties, block = self.count(x,y+1,state,player,liberties, block) #walk south\n",
    "            if x-1 >= 0:\n",
    "                liberties, block = self.count(x-1,y,state,player,liberties, block) #walk west\n",
    "                #if square is empty\n",
    "        elif piece == self.EMPTY:\n",
    "            #mark liberty\n",
    "            state[y][x] = self.LIBERTY\n",
    "            #save liberties\n",
    "            liberties.append((y,x))\n",
    "\n",
    "        return liberties, block\n",
    "    \n",
    "    #remove captured stones\n",
    "    def clear_block(self, block: list, state: list) -> list:\n",
    "        '''\n",
    "        # Description:\n",
    "        Clears the block of stones captured by the opponent on the board.\n",
    "\n",
    "        # Returns:\n",
    "        The board with the captured stones removed.\n",
    "        '''\n",
    "\n",
    "        #clears the elements in the block of elements which is captured\n",
    "        for i in range(len(block)): \n",
    "            y, x = block[i]\n",
    "            state[y][x] = self.EMPTY\n",
    "        \n",
    "        return state\n",
    "    \n",
    "     def print_board(self, state: list) -> None:\n",
    "            '''\n",
    "            # Description:\n",
    "            Draws the board in the console.\n",
    "\n",
    "            # Returns:\n",
    "            None\n",
    "            '''\n",
    "\n",
    "        # Print column coordinates\n",
    "            print(\"   \", end=\"\")\n",
    "            for j in range(len(state[0])):\n",
    "                print(f\"{j:2}\", end=\" \")\n",
    "            print(\"\\n  +\", end=\"\")\n",
    "            for _ in range(len(state[0])):\n",
    "                print(\"---\", end=\"\")\n",
    "            print()\n",
    "\n",
    "            # Print rows with row coordinates\n",
    "            for i in range(len(state)):\n",
    "                print(f\"{i:2}|\", end=\" \")\n",
    "                for j in range(len(state[0])):\n",
    "                    print(f\"{str(int(state[i][j])):2}\", end=\" \")\n",
    "                print()\n",
    "                \n",
    "    def captures(self, state: list,player: int, a:int, b:int) -> tuple[bool, list]:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move causes a capture of stones of the player passed as an argument.\n",
    "        If a move causes a capture, the stones are removed from the board.\n",
    "\n",
    "        # Returns:\n",
    "        A tuple containing a boolean indicating if a capture has been made and the board with the captured stones removed.\n",
    "        '''\n",
    "        check = False\n",
    "        neighbours = []\n",
    "        if(a > 0): neighbours.append((a-1, b))\n",
    "        if(a < self.column_count - 1): neighbours.append((a+1, b))\n",
    "        if(b > 0): neighbours.append((a, b - 1))\n",
    "        if(b < self.row_count - 1): neighbours.append((a, b+1))\n",
    "        #loop over the board squares\n",
    "        for pos in neighbours:\n",
    "            # print(pos)\n",
    "            x = pos[0]\n",
    "            y = pos[1]    \n",
    "            # init piece\n",
    "            piece = state[x][y]\n",
    "\n",
    "                #if stone belongs to given colour\n",
    "            if piece == player:\n",
    "                # print(\"opponent piece\")\n",
    "                # count liberties\n",
    "                liberties = []\n",
    "                block = []\n",
    "                liberties, block = self.count(y, x, state, player, liberties, block)\n",
    "                # print(\"Liberties in count: \" + str(len(liberties)))\n",
    "                # if no liberties remove the stones\n",
    "                if len(liberties) == 0: \n",
    "                    #clear block\n",
    "                    state = self.clear_block(block, state)\n",
    "                    check = True\n",
    "\n",
    "                #restore the board\n",
    "                state = self.restore_board(state)\n",
    "\n",
    "        #print(\"Captures: \" + str(check))\n",
    "        return check, state\n",
    "    \n",
    "    def set_stone(self, a, b, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Places the piece on the board. THIS DOES NOT account for the rules of the game, use get_next_state().\n",
    "\n",
    "        # Retuns:\n",
    "        Board with the piece placed.\n",
    "        '''\n",
    "        state[a][b] = player\n",
    "        return state\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        '''\n",
    "        # Description\n",
    "        Plays the move, verifies and undergoes captures and saves the state to the history.\n",
    "        \n",
    "        # Returns:\n",
    "        New state with everything updated.\n",
    "        '''\n",
    "        if action == self.row_count * self.column_count:\n",
    "            return state # pass move\n",
    "\n",
    "        a = action // self.row_count\n",
    "        b = action % self.column_count\n",
    "\n",
    "        # checking if the move is part of is the secondary move to a ko fight\n",
    "        state = self.set_stone(a, b, state, player)\n",
    "        # print(state)\n",
    "        state = self.captures(state, -player, a, b)[1]\n",
    "        return state\n",
    "    \n",
    "    def is_valid_move(self, state: list, action: tuple, player: int) -> bool:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks if a move is valid.\n",
    "        If a move repeats a previous state or commits suicide (gets captured without capturing back), it is not valid.\n",
    "        \n",
    "        A print will follow explaining the invalid move in case it exists.\n",
    "\n",
    "        # Returns:\n",
    "        A boolean confirming the validity of the move.\n",
    "        '''\n",
    "\n",
    "        a = action[0]\n",
    "        b = action[1]\n",
    "\n",
    "        #print(f\"{a} , {b}\")\n",
    "\n",
    "        statecopy = np.copy(state).astype(np.int8)\n",
    "\n",
    "        if state[a][b] != self.EMPTY:\n",
    "            # print(\"Space Occupied\")\n",
    "            return False\n",
    "        \n",
    "        statecopy = self.set_stone(a,b,statecopy,player)\n",
    "\n",
    "        if self.captures(statecopy, -player, a, b)[0] == True:\n",
    "            return True\n",
    "        else:\n",
    "            #print(\"no captures\")\n",
    "            libs, block = self.count(b,a,statecopy,player,[],[])\n",
    "            #print(libs)\n",
    "            if len(libs) == 0:\n",
    "                #print(\"Invalid, Suicide\")\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "            \n",
    "    def get_valid_moves(self, state, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns a matrix with the valid moves for the current player.\n",
    "        '''\n",
    "        newstate = np.zeros((self.row_count, self.column_count))\n",
    "        for a in range(0, self.column_count):\n",
    "            for b in range(0, self.row_count):\n",
    "                if self.is_valid_move(state, (a,b), player):\n",
    "                    newstate[a][b] = 1\n",
    "        \n",
    "        newstate = newstate.reshape(-1)\n",
    "\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 3: # if 2/3ds are already filled, skipping becomes available\n",
    "                        endgame = False\n",
    "                        break\n",
    "        if endgame:\n",
    "            newstate = np.concatenate([newstate, [1]])\n",
    "        else:\n",
    "            newstate = np.concatenate([newstate, [0]])\n",
    "        return (newstate).astype(np.int8)\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Returns the value of the state and if the game is over.\n",
    "        '''\n",
    "\n",
    "        scoring, endgame = self.scoring(state)\n",
    "\n",
    "        if endgame:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, True\n",
    "                else:\n",
    "                    return -1, True\n",
    "        else:\n",
    "            if player == self.BLACK:\n",
    "                if scoring > 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "            else:\n",
    "                if scoring < 0:\n",
    "                    return 1, False\n",
    "                else:\n",
    "                    return -1, False\n",
    "                \n",
    "                \n",
    "    def scoring(self, state: list) -> int:\n",
    "        '''\n",
    "        # Description:\n",
    "        Checks the score of the game. Score is calculated using:\n",
    "\n",
    "        black - (white + komi)\n",
    "\n",
    "        # Returns:\n",
    "        Integer with score.\n",
    "        '''\n",
    "        black = 0\n",
    "        white = 0\n",
    "        empty = 0\n",
    "        endgame = True\n",
    "\n",
    "        for x in range(self.column_count):\n",
    "            for y in range(self.row_count):\n",
    "                if state[x][y] == self.EMPTY:\n",
    "                    empty += 1\n",
    "                    if empty >= self.column_count * self.row_count // 4:\n",
    "                        endgame = False\n",
    "                        break\n",
    "\n",
    "        black, white = self.count_influenced_territory_enhanced(state)\n",
    "        black_eyes, black_strong_groups = self.count_eyes_and_strong_groups(state, self.BLACK)\n",
    "        white_eyes, white_strong_groups = self.count_eyes_and_strong_groups(state, self.WHITE)\n",
    "        \n",
    "        black += black_eyes + black_strong_groups\n",
    "        white += white_eyes + white_strong_groups\n",
    "        \n",
    "        return black - (white + self.komi), endgame\n",
    "    \n",
    "    \n",
    "    def count_influenced_territory_enhanced(self, board: list) -> tuple[int, int]:\n",
    "        '''\n",
    "        # Description \n",
    "        Calculates the territory influenced by black and white players on the Go board.\n",
    "\n",
    "        This function iterates through the board, analyzing each empty point to determine \n",
    "        if it's influenced by the surrounding black or white stones. The influence is calculated\n",
    "        based on the adjacent stones, with positive scores indicating black influence and negative\n",
    "        scores indicating white influence.\n",
    "\n",
    "        # Returns:\n",
    "        Tuple (black_territory, white_territory)\n",
    "        '''\n",
    "        black_territory = 0\n",
    "        white_territory = 0\n",
    "        visited = set()\n",
    "\n",
    "        # Function to calculate influence score\n",
    "        def influence_score(x, y):\n",
    "            score = 0\n",
    "            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < len(board) and 0 <= ny < len(board[0]):\n",
    "                    score += board[nx][ny]\n",
    "            return score\n",
    "        \n",
    "        # Function to explore territory\n",
    "        def explore_territory(x, y):\n",
    "            nonlocal black_territory, white_territory\n",
    "            if (x, y) in visited or not (0 <= x < len(board) and 0 <= y < len(board[0])):\n",
    "                return\n",
    "            visited.add((x, y))\n",
    "\n",
    "            if board[x][y] == 0:\n",
    "                score = influence_score(x, y)\n",
    "                if score > 0:\n",
    "                    black_territory += 1\n",
    "                elif score < 0:\n",
    "                    white_territory += 1\n",
    "\n",
    "        for i in range(len(board)):\n",
    "            for j in range(len(board[0])):\n",
    "                if board[i][j] == 0 and (i, j) not in visited:\n",
    "                    explore_territory(i, j)\n",
    "\n",
    "        return black_territory, white_territory\n",
    "    \n",
    "    \n",
    "    def is_eye(self, board, x, y, player):\n",
    "\n",
    "        # An eye is an empty point with all adjacent points of the player's color\n",
    "        # and at least one diagonal point of the player's color.\n",
    "        \n",
    "        if board[x][y] != self.EMPTY:\n",
    "            return False\n",
    "        \n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if not (0 <= nx < len(board) and 0 <= ny < len(board[0])):\n",
    "                continue\n",
    "            if board[nx][ny] != player:\n",
    "                return False\n",
    "            \n",
    "        true_eye = False\n",
    "        count = 0\n",
    "        for dx, dy in [(1, 1), (1, -1), (-1, 1), (-1, -1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "\n",
    "            if 0 <= nx < len(board) and 0 <= ny < len(board[0]) and board[nx][ny] == player:\n",
    "                count += 1\n",
    "                if count >= 3:\n",
    "                    true_eye = True\n",
    "\n",
    "\n",
    "        return true_eye\n",
    "    \n",
    "    \n",
    "    def count_eyes_and_strong_groups(self, board, player):\n",
    "        eyes = 0\n",
    "        strong_groups = 0\n",
    "        visited = set()\n",
    "\n",
    "        def dfs(x, y):\n",
    "            if (x, y) in visited or board[x][y] != player:\n",
    "                return 0\n",
    "\n",
    "            visited.add((x, y))\n",
    "            liberties = 0\n",
    "            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if not (0 <= nx < len(board) and 0 <= ny < len(board[0])):\n",
    "                    continue\n",
    "                if board[nx][ny] == self.EMPTY:\n",
    "                    liberties += 1\n",
    "                elif board[nx][ny] == player:\n",
    "                    liberties += dfs(nx, ny)\n",
    "\n",
    "            return liberties\n",
    "        \n",
    "        for x in range(len(board)):\n",
    "            for y in range(len(board[0])):\n",
    "                if board[x][y] == player and (x, y) not in visited:\n",
    "                    liberties = dfs(x, y)\n",
    "                    if liberties >= 2:  # Arbitrary threshold for a strong group\n",
    "                        strong_groups += 1\n",
    "                if board[x][y] != player and (x, y) not in visited and self.is_eye(board, x, y, player):\n",
    "                    eyes += 1\n",
    "\n",
    "        return eyes, strong_groups\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        '''\n",
    "        # Description:\n",
    "        Changes Opponent\n",
    "        '''\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        '''\n",
    "        # Description\n",
    "        Returns the negative value of the value\n",
    "        '''\n",
    "        return -value\n",
    "    \n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        '''\n",
    "        # Description: \n",
    "        Converts the current state of the Go board into a 3-layer encoded format suitable for neural network input.\n",
    "        Each layer in the encoded format represents the presence of a specific type of stone or an empty space on the board:\n",
    "        - Layer 1 encodes the positions of white stones (represented by -1 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 2 encodes the positions of empty spaces (represented by 0 in the input state) as 1s, and all other positions as 0s.\n",
    "        - Layer 3 encodes the positions of black stones (represented by 1 in the input state) as 1s, and all other positions as 0s.\n",
    "        This encoding helps in clearly distinguishing between different elements on the board for machine learning applications.\n",
    "\n",
    "        # Returns: \n",
    "        A NumPy array of shape (3, height, width) containing the 3-layer encoded representation of the board state. Each layer is a 2D array where the board's height and width correspond to the dimensions of the original state.\n",
    "        '''\n",
    "        layer_1 = np.where(np.array(state) == -1, 1, 0).astype(np.float32)\n",
    "        layer_2 = np.where(np.array(state) == 0, 1, 0).astype(np.float32)\n",
    "        layer_3 = np.where(np.array(state) == 1, 1, 0).astype(np.float32)\n",
    "\n",
    "        result = np.stack([layer_1, layer_2, layer_3]).astype(np.float32)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "     def change_perspective(self, state, player):\n",
    "        '''\n",
    "        # Description: \n",
    "        Adjusts the perspective of the Go board state based on the current player.\n",
    "\n",
    "        # Returns: \n",
    "        A two-dimensional array representing the Go board state adjusted for the current player's perspective.\n",
    "        '''\n",
    "        return state * player\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7f95e",
   "metadata": {},
   "source": [
    "### Attaxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f02cf",
   "metadata": {},
   "source": [
    "introducao attaxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae1a92",
   "metadata": {},
   "source": [
    "### Implementação do Attax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "import copy\n",
    "import random as r\n",
    "\n",
    "\n",
    "\n",
    "b_w=\"BLUE WINS!!\"\n",
    "r_w=\"RED WINS!!\"\n",
    "\n",
    "print(\"Escolha número de linhas/colunas:\")\n",
    "NB = int(input())  # Board number of rows/columns\n",
    "size_of_board = 600\n",
    "size_of_square = size_of_board/NB\n",
    "symbol_size = (size_of_square*0.75-10)/2\n",
    "symbol_thickness = 20\n",
    "blue_color = '#496BAB'\n",
    "red_color = '#F33E30'\n",
    "\n",
    "possible_moves_global=[]\n",
    "position_global=[]\n",
    "bool=False\n",
    "origin_pos=[]\n",
    "moves_blue_global=[]\n",
    "moves_red_global=[]\n",
    "blue_pieces=[]\n",
    "red_pieces=[]\n",
    "board2=[]\n",
    "\n",
    "class ataxx():\n",
    "    def __init__(self):\n",
    "        self.window = Tk()\n",
    "        self.window.title('Ataxx')\n",
    "        self.canvas = Canvas(self.window, width=size_of_board, height=size_of_board, background=\"white\")\n",
    "        self.canvas.pack()\n",
    "        self.window.bind('<Button-1>', self.click)\n",
    "        self.board = np.zeros(shape=(NB, NB))\n",
    "        self.board[0][0]=2\n",
    "        self.board[0][NB-1]=1\n",
    "        self.board[NB-1][NB-1]=1\n",
    "        self.board[NB-1][0]=2\n",
    "        self.player_blue_turn = True\n",
    "        self.game_ended = False\n",
    "        self.mode1=0\n",
    "        self.mode2=0\n",
    "        self.init_draw_board()\n",
    "\n",
    "\n",
    "    def mainloop(self):\n",
    "        self.window.mainloop()\n",
    "        if self.mode1==1 and self.mode2==1:\n",
    "            self.ai_vs_ai()\n",
    "            \n",
    "    #----------------DESENHO DO TABULEIRO---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def init_draw_board(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        for i in range(NB-1):\n",
    "            self.canvas.create_line((i+1)*size_of_square, 0, (i+1)*size_of_square, size_of_board)\n",
    "        for i in range(NB-1):\n",
    "            self.canvas.create_line(0,(i+1)*size_of_square, size_of_board, (i+1)*size_of_square)\n",
    "        self.canvas.create_oval(size_of_square/2 - symbol_size, size_of_square/2 - symbol_size,\n",
    "                                size_of_square/2 + symbol_size, size_of_square/2 + symbol_size,\n",
    "                                width=symbol_thickness, outline=red_color,\n",
    "                                fill=red_color)\n",
    "        self.canvas.create_oval(size_of_board - size_of_square/2 - symbol_size,size_of_board - size_of_square/2 - symbol_size,\n",
    "                                size_of_board - size_of_square/2 + symbol_size, size_of_board - size_of_square/2 + symbol_size,\n",
    "                                width=symbol_thickness, outline=blue_color,\n",
    "                                fill=blue_color)\n",
    "        self.canvas.create_oval(size_of_square/2 - symbol_size,size_of_board - size_of_square/2 - symbol_size,\n",
    "                                size_of_square/2 + symbol_size, size_of_board - size_of_square/2 + symbol_size,\n",
    "                                width=symbol_thickness, outline=blue_color,\n",
    "                                fill=blue_color)\n",
    "        self.canvas.create_oval(size_of_board - size_of_square/2 - symbol_size, size_of_square/2- symbol_size,\n",
    "                                size_of_board - size_of_square/2 + symbol_size, size_of_square/2 + symbol_size,\n",
    "                                width=symbol_thickness, outline=red_color,\n",
    "                                fill=red_color)\n",
    "\n",
    "\n",
    "    def update_board(self, x, y, origin):\n",
    "        for i in range(max(0, x-1), min(NB, x+2)):\n",
    "            for j in range(max(0, y-1), min(NB, y+2)):\n",
    "                if not self.is_square_clear([i,j]):\n",
    "                    if self.player_blue_turn:\n",
    "                        self.draw_blue([i,j])\n",
    "                    else:\n",
    "                        self.draw_red([i,j])\n",
    "                    self.board[i][j]=self.board[x][y]\n",
    "        if x-origin[0]== 2 or y-origin[1]== 2 or x-origin[0]== -2 or y-origin[1]== -2:\n",
    "            self.board[origin[0]][origin[1]]=0\n",
    "            pos=self.convert_logical_to_grid_position(origin)\n",
    "            self.draw_whitespace(pos)\n",
    "        self.score()\n",
    "        self.all_moves()\n",
    "        self.player_blue_turn = not self.player_blue_turn\n",
    "\n",
    "    def update_board2(self, board, x, y, origin):\n",
    "        for i in range(max(0, x-1), min(NB, x+2)):\n",
    "            for j in range(max(0, y-1), min(NB, y+2)):\n",
    "                if not board[pos[0]][pos[1]] == 0:\n",
    "                    board[i][j]=board[x][y]\n",
    "        if x-origin[0]== 2 or y-origin[1]== 2 or x-origin[0]== -2 or y-origin[1]== -2:\n",
    "            board[origin[0]][origin[1]]=0\n",
    "\n",
    "    def all_moves(self):\n",
    "        global moves_blue_global\n",
    "        global moves_red_global\n",
    "        global blue_pieces\n",
    "        global red_pieces\n",
    "        for i in range(NB):\n",
    "            for j in range(NB):\n",
    "                if self.board[i][j]==1:\n",
    "                    moves_blue_global.append(self.possible_moves([i,j]))\n",
    "                    blue_pieces.append([i,j])\n",
    "                elif self.board[i][j]==2:\n",
    "                    moves_red_global.append(self.possible_moves([i,j]))\n",
    "                    red_pieces.append([i,j])\n",
    "        if len(moves_blue_global)==0:\n",
    "            self.no_moves(1)\n",
    "        elif len(moves_red_global)==0:\n",
    "            self.no_moves(2)\n",
    "        moves_blue_global=[]\n",
    "        moves_red_global=[]\n",
    "\n",
    "    def no_moves(self, player):\n",
    "        print(\"3\")\n",
    "        if player==1:\n",
    "            for i in range(NB):\n",
    "                for j in range(NB):\n",
    "                    if self.board[i][j]==0:\n",
    "                        self.board[i][j]=2\n",
    "                        self.draw_red([i,j])\n",
    "        elif player==2:\n",
    "            for i in range(NB):\n",
    "                for j in range(NB):\n",
    "                    if self.board[i][j]==0:\n",
    "                        self.board[i][j]=1\n",
    "                        self.draw_blue([i,j])\n",
    "        self.score()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def execute_move(self, move, origin, player):\n",
    "\n",
    "        \n",
    "        self.board[move[0]][move[1]] = player\n",
    "        self.update_board(move[0], move[1], origin)\n",
    "\n",
    "    def is_square_clear(self, pos):\n",
    "        if not np.array_equal(pos, []):\n",
    "            return self.board[pos[0]][pos[1]] == 0\n",
    "\n",
    "    def valid_move(self, logical_pos):\n",
    "        return self.is_square_clear(logical_pos)\n",
    "\n",
    "    def possible_moves(self, move):\n",
    "\n",
    "    #dado a peça selecionada, devolve uma lista\n",
    "    #com todos os movimentos possiveis da mesma \n",
    "\n",
    "        possible_moves=[]\n",
    "        for i in range(max(0,move[0]-2), min(NB, move[0]+3)):\n",
    "            for j in range(max(0,move[1]-2), min(NB, move[1]+3)):\n",
    "                \n",
    "                if self.is_square_clear([i,j]):\n",
    "                    possible_moves.append([i,j])\n",
    "        \n",
    "        return possible_moves\n",
    "\n",
    "    def score(self):\n",
    "        cont_blue=0\n",
    "        cont_red=0\n",
    "        cheio=True\n",
    "        for i in range(NB):\n",
    "            for j in range(NB):\n",
    "                if self.board[i][j]==1:\n",
    "                    cont_blue+=1\n",
    "                elif self.board[i][j]==2:\n",
    "                    cont_red+=1\n",
    "                if self.board[i][j]==0:\n",
    "                    cheio=False\n",
    "        print(\"Blue score= \",  cont_blue)\n",
    "        print(\"Red score= \",  cont_red)\n",
    "        print(\"------------------------\")\n",
    "        self.window.title(\"Ataxx - Red : %d vs %d : Blue\" % (cont_red, cont_blue))\n",
    "        if cont_blue==0:\n",
    "            self.game_is_over(cont_red, cont_blue)\n",
    "        elif cont_red==0:\n",
    "            self.game_is_over(cont_red, cont_blue)\n",
    "        elif cheio:\n",
    "            self.game_is_over(cont_red, cont_blue)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def game_is_over(self, red, blue):\n",
    "        print(\"Blue score= \",  blue)\n",
    "        print(\"Red score= \",  red)\n",
    "        print(\"\\n\")\n",
    "        if blue>red:\n",
    "            print(b_w)\n",
    "            a=1\n",
    "        else:\n",
    "            print(r_w)\n",
    "            a=2\n",
    "        print(\"\\n\")\n",
    "        self.clear_possible_moves()\n",
    "        self.window.destroy()\n",
    "\n",
    "\n",
    "#----------------------TRANSFORMAR EM MATRIZ PARA APLICAR REGRAS---------------\n",
    "\n",
    "    def convert_logical_to_grid_position(self, logical_pos):\n",
    "        logical_pos = np.array(logical_pos, dtype=int)\n",
    "        return np.array((size_of_square)*logical_pos + size_of_square/2)\n",
    "\n",
    "    def convert_grid_to_logical_position(self, grid_pos):\n",
    "        grid_pos = np.array(grid_pos)\n",
    "        return np.array(grid_pos//size_of_square, dtype=int)\n",
    "\n",
    "#-----------------------DESENHAR PECAS----------------------------------------\n",
    "    def draw_whitespace(self, grid_pos):\n",
    "\n",
    "        \n",
    "\n",
    "        self.canvas.create_rectangle(grid_pos[0] - symbol_size, grid_pos[1] - symbol_size,\n",
    "                            grid_pos[0] + symbol_size, grid_pos[1] + symbol_size,\n",
    "                            width=symbol_thickness, outline=\"white\",\n",
    "                            fill=\"white\")\n",
    "\n",
    "\n",
    "    def draw_blue(self, logical_pos):\n",
    "        logical_pos = np.array(logical_pos)\n",
    "        grid_pos = self.convert_logical_to_grid_position(logical_pos)\n",
    "        self.canvas.create_oval(grid_pos[0] - symbol_size, grid_pos[1] - symbol_size,\n",
    "                            grid_pos[0] + symbol_size, grid_pos[1] + symbol_size,\n",
    "                            width=symbol_thickness, outline=blue_color,\n",
    "                            fill=blue_color)\n",
    "\n",
    "    def draw_red(self, logical_pos):\n",
    "        logical_pos = np.array(logical_pos)\n",
    "        grid_pos = self.convert_logical_to_grid_position(logical_pos)\n",
    "        self.canvas.create_oval(grid_pos[0] - symbol_size, grid_pos[1] - symbol_size,\n",
    "                            grid_pos[0] + symbol_size, grid_pos[1] + symbol_size,\n",
    "                            width=symbol_thickness, outline=red_color,\n",
    "                            fill=red_color)\n",
    "\n",
    "    def draw_possible_moves(self, possible_moves):\n",
    "\n",
    "        # desenha no tabuleiro as jogadas possiveis da bola selecionada\n",
    "\n",
    "        moves=[0]*len(possible_moves)\n",
    "        for i in range(len(possible_moves)):\n",
    "            moves[i]=self.convert_logical_to_grid_position(possible_moves[i])\n",
    "            self.canvas.create_oval(moves[i][0]-symbol_size, moves[i][1] - symbol_size,\n",
    "                                    moves[i][0]+symbol_size, moves[i][1]+ symbol_size,\n",
    "                                    width=symbol_thickness, outline=\"gray\", fill=\"gray\", tags=\"possible\")\n",
    "\n",
    "\n",
    "    def clear_possible_moves(self):\n",
    "\n",
    "        \n",
    "\n",
    "        self.canvas.delete(\"possible\")\n",
    "        \n",
    "#----------------------------------- Verificaçao movimentos e jogadas ------------------------------------\n",
    "\n",
    "    def total_moves(board, player,ROWS):\n",
    "        moves = []\n",
    "        moves_aval = []\n",
    "        for peca in totalpecas(board, ROWS, player):\n",
    "            moves_aval = plays_eval(peca,ROWS,board)\n",
    "            for move in moves_aval:\n",
    "                temp_board = deepcopy(board)\n",
    "                temp_peca = (peca[0],peca[1])\n",
    "                new_board = simula_move(temp_peca, move, temp_board, player, ROWS)\n",
    "                moves.append(new_board)\n",
    "        return moves\n",
    "\n",
    "#----------------------- MOUSE -----------------------------------------------------------\n",
    "\n",
    "    def click(self, event):        \n",
    "        if self.game_ended: return\n",
    "        grid_pos = [event.x, event.y]\n",
    "        logical_pos = self.convert_grid_to_logical_position(grid_pos)\n",
    "        global origin_pos\n",
    "        global possible_moves_global\n",
    "        origin_pos = logical_pos\n",
    "        \n",
    "        if self.board[logical_pos[0]][logical_pos[1]] == 1 and self.player_blue_turn:\n",
    "            possible_moves_global = self.possible_moves(logical_pos)\n",
    "            \n",
    "            if not np.array_equal(possible_moves_global, []):\n",
    "                self.window.bind(\"<Button-1>\", self.second_click)\n",
    "            self.draw_possible_moves(possible_moves_global)\n",
    "            \n",
    "        elif self.board[logical_pos[0]][logical_pos[1]] == 2 and not self.player_blue_turn:\n",
    "            possible_moves_global = self.possible_moves(logical_pos)\n",
    "            \n",
    "            if not np.array_equal(possible_moves_global, []):\n",
    "                self.window.bind(\"<Button-1>\", self.second_click)\n",
    "            self.draw_possible_moves(possible_moves_global)\n",
    "\n",
    "\n",
    "    def second_click(self, event):\n",
    "        global bool\n",
    "        grid_pos = [event.x, event.y]\n",
    "        logical_pos = self.convert_grid_to_logical_position(grid_pos)\n",
    "        global possible_moves_global\n",
    "        possible_moves_global=np.array(possible_moves_global, dtype=int)\n",
    "        \n",
    "        for element in possible_moves_global:\n",
    "            \n",
    "            if np.array_equal(logical_pos, element):\n",
    "                global position_global\n",
    "                position_global = logical_pos\n",
    "                bool=True\n",
    "        bool = True\n",
    "        self.click2()\n",
    "        possible_moves_global=[]\n",
    "        position_global=[]\n",
    "\n",
    "\n",
    "    def click2(self):\n",
    "        global bool\n",
    "        \n",
    "        if self.player_blue_turn:\n",
    "            player=1\n",
    "        else:\n",
    "            player=2\n",
    "            \n",
    "        if self.valid_move(position_global):\n",
    "            \n",
    "            if self.second_click_pressed(bool):\n",
    "                if self.player_blue_turn and self.board[origin_pos[0]][origin_pos[1]] == 1:\n",
    "                    self.draw_blue(position_global)\n",
    "                    self.execute_move(position_global, origin_pos, player)\n",
    "                    \n",
    "                elif not self.player_blue_turn and self.board[origin_pos[0]][origin_pos[1]] == 2:\n",
    "                    self.draw_red(position_global)\n",
    "                    self.execute_move(position_global, origin_pos, player)\n",
    "        self.clear_possible_moves()\n",
    "        self.window.bind(\"<Button-1>\", self.click)\n",
    "        \n",
    "\n",
    "    def second_click_pressed(self, bool):\n",
    "        if bool:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def PvsP():\n",
    "    game = ataxx()\n",
    "    game.mainloop()\n",
    "   \n",
    "\n",
    "PvsP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757cc09",
   "metadata": {},
   "source": [
    "### MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de707234",
   "metadata": {},
   "source": [
    "explicação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896b30a",
   "metadata": {},
   "source": [
    "### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68be3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    # Alpha Zero Node\n",
    "    ## Description:\n",
    "        A node for the AlphaZero MCTS. It contains the state, the action taken to get to the state, the prior probability of the action, the visit count, the value sum, and the children of the node.\n",
    "    ## Metohds:\n",
    "        - `is_expanded()`: Returns whether the node has been expanded.\n",
    "        - `select()`: Selects the best child node based on the UCB.\n",
    "        - `get_ucb()`: Returns the UCB of a child node.\n",
    "        - `expand()`: Expands the node by adding children.\n",
    "        - `backpropagate()`: Backpropagates the value of the node to the parent node.\n",
    "        '''\n",
    "    def __init__(self, game, args, state, player, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.player = player\n",
    "        self.prior = prior\n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_expanded(self):\n",
    "        '''\n",
    "        # is_expanded\n",
    "        ## Description:\n",
    "            Returns whether the node has been expanded.\n",
    "        ## Returns:\n",
    "            - `bool`: Whether the node has been expanded.'''\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        '''\n",
    "        # Description: \n",
    "        Selects the best child node from the current node's children in a Monte Carlo Tree Search using the Upper Confidence Bound (UCB) algorithm. \n",
    "\n",
    "        # Returns: \n",
    "        The best child node, chosen based on the highest UCB value or randomly if there's a tie.\n",
    "        '''\n",
    "        best_child = []\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = [child]\n",
    "                best_ucb = ucb\n",
    "            elif ucb == best_ucb:\n",
    "                best_child.append(child)\n",
    "                \n",
    "        return best_child[0] if len(best_child) == 1 else random.choice(best_child)\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        '''\n",
    "        # Description: \n",
    "        Calculates the Upper Confidence Bound (UCB) value for a given child node in a Monte Carlo Tree Search.\n",
    "\n",
    "        # Returns: \n",
    "        The calculated UCB value for the given child node.\n",
    "        '''\n",
    "        if child.visit_count == 0:\n",
    "            q_value = child.prior * self.args['C'] * (math.sqrt(self.visit_count)) / (child.visit_count + 1)\n",
    "        else:\n",
    "            q_value = -(child.value_sum / child.visit_count) + child.prior * self.args['C'] * (math.sqrt(self.visit_count)) / (child.visit_count + 1)\n",
    "        return q_value\n",
    "    \n",
    "    def serialize(self):\n",
    "        # Serialize only essential data\n",
    "        node_data = {\n",
    "            'game': self.game,\n",
    "            'args': self.args,\n",
    "            'parent': self.parent,\n",
    "            'state': self.state,\n",
    "            'action_taken': self.action_taken,\n",
    "            'player': self.player,\n",
    "            'prior': self.prior,\n",
    "            'visit_count': self.visit_count,\n",
    "            'value_sum': self.value_sum,\n",
    "            'children': [child for child in self.children]  # Assuming each child has a unique ID\n",
    "        }\n",
    "        return json.dumps(node_data)\n",
    "    \n",
    "    def deserialize(node_json):\n",
    "        # Convert JSON back into a Node object\n",
    "        node_data = json.loads(node_json)\n",
    "        node = Node(  # assuming constructor can handle this data\n",
    "            game = node_data['game'],\n",
    "            args = node_data['args'],\n",
    "            parent = node_data['parent'],\n",
    "            player = node_data['player'],\n",
    "            state=node_data['state'],\n",
    "            action_taken=node_data['action_taken'],\n",
    "            prior=node_data['prior'],\n",
    "            visit_count=node_data['visit_count'],\n",
    "        )\n",
    "        node.value_sum = node_data['value_sum']\n",
    "\n",
    "        for child in node_data['children']:\n",
    "            child.parent = node\n",
    "            node.children.append(child)\n",
    "\n",
    "        # You'll need to handle children reconstruction separately\n",
    "        return node\n",
    "    def expand(self, policy):\n",
    "        '''\n",
    "        # Description: \n",
    "        Expands the current node by adding new child nodes based on the given policy probabilities. For each possible action, it calculates the next state, adjusts the perspective for the opponent, and creates a new child node if the probability for that action is greater than zero.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "                child = Node(self.game, self.args, child_state, self.game.get_opponent(self.player), self, action, prob)\n",
    "                self.children.append(child)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        '''\n",
    "        # Description: \n",
    "        Performs the backpropagation step in Monte Carlo Tree Search. It updates the current node's value sum and visit count based on the received value.\n",
    "\n",
    "        # Returns: \n",
    "        None\n",
    "        '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            self.parent.backpropagate(value)\n",
    "            \n",
    "            \n",
    "class MCTS:\n",
    "    def __init__(self, model, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.tree_dict = {}\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, player):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Performs Monte Carlo Tree Search (MCTS) in batch to find the best action.\n",
    "\n",
    "        # Returns:\n",
    "        An array of arrays of action probabilities for each possible action.\n",
    "        \"\"\"\n",
    "        roots = []\n",
    "        for state in states:\n",
    "            root = Node(self.game, self.args, state, player, visit_count=1)\n",
    "\n",
    "            searches = self.args['num_mcts_searches']\n",
    "\n",
    "            # if str(state)+str(player) not in self.tree_dict.keys():\n",
    "            #     self.tree_dict.update({str(state)+str(player): root.serialize()})\n",
    "\n",
    "            # else: # the state is already in the dictionary\n",
    "            #    root = Node.deserialize(self.tree_dict.get(str(state)+str(player)))\n",
    "            #    searches = (searches // 4)\n",
    "\n",
    "            roots.append(root)\n",
    "            \n",
    "            policy, _ = self.model(\n",
    "                torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "            )\n",
    "            policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "            policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "                * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "                \n",
    "            valid_moves = self.game.get_valid_moves(state, player)\n",
    "\n",
    "            if self.args[\"game\"] == \"Attaxx\":\n",
    "                if np.sum(valid_moves) == 0:\n",
    "                    valid_moves[-1] = 1\n",
    "                else:\n",
    "                    valid_moves[-1] = 0\n",
    "                    policy *= valid_moves\n",
    "            policy /= np.sum(policy)\n",
    "            root.expand(policy)\n",
    "                \n",
    "            for search in range(searches):\n",
    "                node = root\n",
    "                while node.is_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken, node.player)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                    \n",
    "                if node.parent is not None:\n",
    "                    if node.action_taken == self.game.action_size - 1 and node.parent.action_taken == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                        is_terminal = True # if the action is pass when the previous action was also pass, end the game\n",
    "\n",
    "                if not is_terminal:\n",
    "                    policy, value = self.model(torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0))\n",
    "                    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                    valid_moves = self.game.get_valid_moves(node.state, player)\n",
    "\n",
    "                    if self.args[\"game\"] == \"Attaxx\":\n",
    "                        if np.sum(valid_moves) == 0:\n",
    "                            valid_moves[-1] = 1\n",
    "                        else:\n",
    "                            valid_moves[-1] = 0\n",
    "\n",
    "                    policy *= valid_moves\n",
    "                    policy /= np.sum(policy)\n",
    "                        \n",
    "                    value = value.item()\n",
    "                    node.expand(policy)\n",
    "\n",
    "                node.backpropagate(value)\n",
    "                \n",
    "        action_prob_list = []\n",
    "\n",
    "        for root in roots:\n",
    "            action_probs = np.zeros(self.game.action_size)\n",
    "            for child in root.children:\n",
    "                action_probs[child.action_taken] = child.visit_count\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            action_prob_list.append(action_probs)\n",
    "\n",
    "        return action_prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b865",
   "metadata": {},
   "source": [
    "### Rede Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc01e75",
   "metadata": {},
   "source": [
    "Introdução de uma Rede Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0777568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    # ResNet\n",
    "    ## Description:\n",
    "        A ResNet model for AlphaZero.\n",
    "        The model takes in a state and outputs a policy and value.\n",
    "         - The policy is a probability distribution over all possible actions.\n",
    "         - The value is a number between -1 and 1, where -1 means the current player loses and 1 means the current player wins following a tanh activation.\n",
    "        '''\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=\"same\"),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        # Description:\n",
    "        The forward pass of the model. This overrides the forward method of nn.Module so that it can be called directly on the model.\n",
    "\n",
    "        # Returns:\n",
    "        - `policy`: The policy output of the model.\n",
    "        - `value`: The value output of the model.\n",
    "        '''\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "    # Description:\n",
    "    A residual block for the ResNet model.\n",
    "    '''\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # Description:\n",
    "        Forward pass through the residual block.\n",
    "\n",
    "        # Returns:\n",
    "        Output tensor after passing through the block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdea121",
   "metadata": {},
   "source": [
    "### AlphaZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0a1bf",
   "metadata": {},
   "source": [
    "Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(model, game, args)\n",
    "\n",
    "    def augment_state(self, state, probs):\n",
    "\n",
    "        augmented_states = []\n",
    "\n",
    "        skip_prob = probs[-1]\n",
    "        action_probs_matrix = np.array(probs[:-1]).reshape(self.game.column_count, self.game.row_count)\n",
    "        augmented_action_probs = []\n",
    "\n",
    "        def augment_and_append(transformed_state, transformed_probs_matrix):\n",
    "\n",
    "            # Append state\n",
    "            augmented_states.append(transformed_state)\n",
    "\n",
    "            # Flatten probs matrix, append the last value, and then append to augmented_action_probs\n",
    "            augmented_action_probs.append(list(transformed_probs_matrix.flatten()) + [skip_prob])\n",
    "\n",
    "        # Original state and probs\n",
    "        augment_and_append(state, action_probs_matrix)\n",
    "        # Rotate 90 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=1), np.rot90(action_probs_matrix, k=1))\n",
    "\n",
    "        # Rotate 180 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=2), np.rot90(action_probs_matrix, k=2))\n",
    "\n",
    "        # Rotate 270 degrees clockwise\n",
    "        augment_and_append(np.rot90(state, k=3), np.rot90(action_probs_matrix, k=3))\n",
    "\n",
    "        # Flip horizontally\n",
    "        augment_and_append(np.fliplr(state), np.fliplr(action_probs_matrix))\n",
    "\n",
    "        # Flip vertically\n",
    "        augment_and_append(np.flipud(state), np.flipud(action_probs_matrix))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip horizontally\n",
    "        augment_and_append(np.rot90(np.fliplr(state), k=1), np.rot90(np.fliplr(action_probs_matrix), k=1))\n",
    "\n",
    "        # Rotate 90 degrees clockwise and flip vertically\n",
    "        augment_and_append(np.rot90(np.flipud(state), k=1), np.rot90(np.flipud(action_probs_matrix), k=1))\n",
    "\n",
    "        return augmented_states, augmented_action_probs\n",
    "    \n",
    "    def selfPlay(self):\n",
    "        player = 1\n",
    "\n",
    "        memory = []\n",
    "        states = []\n",
    "\n",
    "        for _ in range(0, self.args['parallel_games']):\n",
    "            state = self.game.get_initial_state()\n",
    "            states.append(state)\n",
    "            memory.append([])\n",
    "\n",
    "        iter = 0\n",
    "        prev_skip = False\n",
    "        temperature = self.args['temperature']\n",
    "        debugging = False\n",
    "\n",
    "        returnData = []\n",
    "        while True:\n",
    "            if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                print(\"\\nSEARCHING...\")\n",
    "\n",
    "            neutral_states_list = []\n",
    "\n",
    "            for state in states:\n",
    "                neutral_states_list.append(self.game.change_perspective(state, player))\n",
    "\n",
    "            action_probs_list = self.mcts.search(states, player)\n",
    "\n",
    "            for i, (neutral_state, action_probs) in enumerate(zip(neutral_states_list, action_probs_list)):\n",
    "                memory[i].append((neutral_state, action_probs, player))\n",
    "\n",
    "            for idx, (state, action_probs) in enumerate(zip(states, action_probs_list)):\n",
    "                temperature_action_probs = action_probs ** (1 / temperature)\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                \n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "                if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                    print(f\"Player: {player} with move {self.game.int_to_move(action)}\\nBoard:\")\n",
    "                    self.game.print_board(state)    \n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(state, action, player)\n",
    "                    \n",
    "\n",
    "                if action == self.game.action_size - 1 and self.args['game'] == 'Go':\n",
    "                    if prev_skip:\n",
    "                        is_terminal = True\n",
    "                    else:\n",
    "                        prev_skip = True\n",
    "                else:\n",
    "                    prev_skip = False\n",
    "\n",
    "                if is_terminal or iter >= self.args['max_moves']:\n",
    "                    returnMemory = []\n",
    "                    if self.args[\"game\"] == \"Attaxx\" and debugging:\n",
    "                        print(\"GAME OVER\\n\\n\")\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in memory[idx]:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "\n",
    "                        if self.args['augment']:\n",
    "                            augmented_states, augmented_action_probs = self.augment_state(hist_neutral_state, hist_action_probs)\n",
    "\n",
    "                            for augmented_state, augmented_probs in zip(augmented_states, augmented_action_probs):\n",
    "                                returnMemory.append((self.game.get_encoded_state(augmented_state), augmented_probs, hist_outcome))\n",
    "                        else:\n",
    "                            returnMemory.append((self.game.get_encoded_state(hist_neutral_state), hist_action_probs, hist_outcome))\n",
    "\n",
    "                        returnData = returnData + returnMemory\n",
    "                        \n",
    "                        del memory[idx]\n",
    "                    del states[idx]\n",
    "\n",
    "                if len(memory) <= 0:\n",
    "                    return returnData\n",
    "\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "            if temperature >= 0.1:\n",
    "                temperature = temperature * self.args['cooling_constant']\n",
    "            else:\n",
    "                temperature = 0.1\n",
    "\n",
    "            iter += 1\n",
    "            \n",
    "            \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def learn(self, memory = None, LAST_ITERATION=0):\n",
    "        primary_memory = []\n",
    "\n",
    "        if memory != None:\n",
    "            primary_memory = memory\n",
    "\n",
    "        for iteration in range(LAST_ITERATION+1, self.args['num_iterations']):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "\n",
    "            secondary_memory = []\n",
    "\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                states = self.selfPlay()\n",
    "                secondary_memory += states\n",
    "\n",
    "            training_memory = []\n",
    "            if self.args['experience_replay']:\n",
    "                sample_size = int(len(primary_memory) * 0.3)\n",
    "\n",
    "                training_memory += random.sample(primary_memory, min(sample_size, len(primary_memory)))\n",
    "                training_memory += secondary_memory\n",
    "                \n",
    "                primary_memory += secondary_memory\n",
    "            else:\n",
    "                training_memory += secondary_memory\n",
    "\n",
    "            print(f\"Memory size: {len(training_memory)}\")\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(training_memory)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"DevelopmentModels/{self.args['alias']}/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"DevelopmentModels/{self.args['alias']}/optimizer_{iteration}.pt\")\n",
    "            with open(f'DevelopmentModels/{self.args[\"alias\"]}/memory_{iteration}.pkl', 'wb') as f:\n",
    "                pickle.dump(primary_memory, f)\n",
    "            print(\"Data Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33df871",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a572fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "SAVE_NAME = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Go / Attaxx\n",
    "    GAME = \"Attaxx\"\n",
    "\n",
    "    # Board size (7/9 for Go, 4/5/6 for Attaxx)\n",
    "    SIZE = 6\n",
    "    # True to load previous model\n",
    "    # False to start from scratch\n",
    "    LOAD = True\n",
    "    LAST_ITERATION = 1\n",
    "\n",
    "    # Save Name\n",
    "    SAVE_NAME = \"4x4Parallel_5\"\n",
    "\n",
    "    # False for training\n",
    "    # True for playing\n",
    "    TEST = True\n",
    "\n",
    "    # False if locally \n",
    "    # True if playing in the server\n",
    "    ONLINE = False\n",
    "\n",
    "    # Train from scratch\n",
    "    if not LOAD and not TEST:\n",
    "        LAST_ITERATION=-1\n",
    "        \n",
    "        \n",
    "    if GAME == 'Go':\n",
    "        if SIZE == 7:\n",
    "            args = {\n",
    "                'game': 'Go',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 15,    # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 20,                 # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 16,                 # batch size for training\n",
    "                'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.90,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 2,                           # the value of the constant policy\n",
    "                'experience_replay': True,        # recycle a certain % of old random selfplay data in the current training iteration\n",
    "                'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                'parallel_games': 10,            # number of games run in parallel\n",
    "                'dirichlet_alpha': 0.03,          # the value of the dirichlet noise (alpha)\n",
    "                'dirichlet_epsilon': 0.25,        # the value of the dirichlet noise (epsilon)\n",
    "                'alias': ('Go' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Go(size = SIZE, komi = 5.5)\n",
    "            model = ResNet(game, 10, 10, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "        elif SIZE == 9:\n",
    "            args = {\n",
    "                'game': 'Go',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,    # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 200,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 60,                 # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 32,                 # batch size for training\n",
    "                'temperature': 3,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 2,                           # the value of the constant policy\n",
    "                'experience_replay': True,        # recycle a certain % of old random selfplay data in the current training iteration\n",
    "                'augment': False,                 # whether to augment the training data with flipped and rotated states\n",
    "                'parallel_games': 5,            # number of games run in parallel\n",
    "                'dirichlet_alpha': 0.032,          # the value of the dirichlet noise (alpha)\n",
    "                'dirichlet_epsilon': 0.25,        # the value of the dirichlet noise (epsilon)\n",
    "                'alias': ('Go' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Go(size = SIZE, komi = 5.5)\n",
    "            model = ResNet(game, 15, 15, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "    elif GAME == 'Attaxx':\n",
    "        game_size = [SIZE,SIZE]\n",
    "        if SIZE == 4:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 20,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 100,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 10,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 16,                # batch size for training\n",
    "                'temperature': 2,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.9,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 4,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.3,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.2,       # the 001value of the dirichlet noise\n",
    "                'parallel_games': 10,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 4, 8, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "        elif SIZE == 5:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 10000,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 100,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 10,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 64,                # batch size for training\n",
    "                'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 4,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.3,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.2,       # the value of the dirichlet noise\n",
    "                'parallel_games': 15,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 8, 16, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif SIZE == 6:\n",
    "            args = {\n",
    "                'game': 'Attaxx',\n",
    "                'num_iterations': 10000,             # number of highest level iterations\n",
    "                'num_selfPlay_iterations': 20,  # number of self-play games to play within each iteration\n",
    "                'num_mcts_searches': 150,         # number of mcts simulations when selecting a move within self-play\n",
    "                'max_moves': 512,                 # maximum number of moves in a game (to avoid infinite games which should not happen but just in case)\n",
    "                'num_epochs': 20,                # number of epochs for training on self-play data for each iteration\n",
    "                'batch_size': 64,                # batch size for training\n",
    "                'temperature': 1,                 # temperature for the softmax selection of moves\n",
    "                'cooling_constant': 0.85,         # value that gets multiplied to the temperature to gradually reduce it  \n",
    "                'C': 4,                           # the value of the constant policy\n",
    "                'dirichlet_alpha': 0.3,           # the value of the dirichlet noise\n",
    "                'dirichlet_epsilon': 0.2,       # the value of the dirichlet noise\n",
    "                'parallel_games': 20,            # number of games run in parallel\n",
    "                'experience_replay': True,        # we recycle 30% of old random selfplay data in the current training iteration\n",
    "                'augment': False,                  # whether to augment the training data with flipped and rotated states\n",
    "                'alias': ('Attaxx' + SAVE_NAME)\n",
    "            }\n",
    "\n",
    "            game = Attaxx(game_size)\n",
    "            model = ResNet(game, 12, 32, device)\n",
    "            optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    else:\n",
    "        print(\"Game Unavailable\")\n",
    "        \n",
    "        \n",
    "    if LOAD:\n",
    "        model.load_state_dict(torch.load(f'DevelopmentModels/{GAME+SAVE_NAME}/model_{LAST_ITERATION}.pt', map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(f'DevelopmentModels/{GAME+SAVE_NAME}/optimizer_{LAST_ITERATION}.pt', map_location=device))\n",
    "    \n",
    "        #with open(f'DevelopmentModels/{GAME+SAVE_NAME}/memory_{LAST_ITERATION}.pkl', 'rb') as f:\n",
    "         #   memory = pickle.load(f)\n",
    "    else:\n",
    "        memory = None\n",
    "\n",
    "    if not TEST:\n",
    "        os.makedirs(f'DevelopmentModels/{GAME+SAVE_NAME}', exist_ok=True)\n",
    "        alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "        alphaZero.learn(memory, LAST_ITERATION)\n",
    "        \n",
    "        \n",
    "    elif not ONLINE:\n",
    "\n",
    "        if not LOAD:\n",
    "            print(\"No model to test\")\n",
    "            exit()\n",
    "\n",
    "        if GAME == 'Go':\n",
    "            PLAYER1 = \"user\"\n",
    "            PLAYER2 = \"AI\"\n",
    "            game = Go(SIZE, 5.5)\n",
    "\n",
    "            model.load_state_dict(torch.load(f'DevelopmentModels/{GAME+SAVE_NAME}/model_{LAST_ITERATION}.pt'), map_location = device)\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            game.print_board(state)\n",
    "\n",
    "            player = 1\n",
    "            prev_skip = False\n",
    "            while True:\n",
    "                if player == 1:\n",
    "                    if PLAYER1 == 'user':\n",
    "                        a, b = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                        print(\"\\n\")\n",
    "                        action = a * SIZE + b\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                    else:\n",
    "                        tmp_state = game.change_perspective(state, -1)\n",
    "                        action = mcts.search([tmp_state], -player)                    \n",
    "                        action = np.argmax(action[0])\n",
    "\n",
    "                        print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                else:\n",
    "                    if PLAYER2 == 'user':\n",
    "                        a, b = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                        print(\"\\n\")\n",
    "                        action = a * SIZE + b\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                        \n",
    "                    else:\n",
    "                        action = mcts.search([tmp_state], -player)                    \n",
    "                        action = np.argmax(action[0])\n",
    "\n",
    "                        print(f\"\\nAlphaZero Action: {action // game.row_count} {action % game.column_count}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "\n",
    "                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                \n",
    "                if action == game.action_size:\n",
    "                    if prev_skip:\n",
    "                        win = True\n",
    "                    else:\n",
    "                        prev_skip = True\n",
    "                else:\n",
    "                    prev_skip = False\n",
    "\n",
    "                if win:\n",
    "                    game.print_board(state)\n",
    "                    print(f\"player {winner} wins\")\n",
    "                    exit()\n",
    "\n",
    "                player = - player\n",
    "                game.print_board(state)\n",
    "                \n",
    "        elif GAME == 'Attaxx':\n",
    "            PLAYER1 = \"AI\"\n",
    "            PLAYER2 = \"AI\"\n",
    "            game = Attaxx(game_size)\n",
    "\n",
    "            model.load_state_dict(torch.load(f'DevelopmentModels/{GAME+SAVE_NAME}/model_{LAST_ITERATION}.pt', map_location = device))\n",
    "            mcts = MCTS(model, game, args)\n",
    "            state = game.get_initial_state()\n",
    "            game.print_board(state)\n",
    "\n",
    "            player = 1\n",
    "            \n",
    "            \n",
    "        while True:\n",
    "                if player == 1:\n",
    "                    if PLAYER1 == 'user':\n",
    "                        move = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                        print(\"\\n\")\n",
    "                        action = game.move_to_int(move)\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                    else:\n",
    "                        tmp_state = game.change_perspective(state, -1)\n",
    "                        action = mcts.search([tmp_state], -player)\n",
    "                        action = np.argmax(action)\n",
    "                        print(f\"\\nAlphaZero Action: {game.int_to_move(action)}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                else:\n",
    "                    if PLAYER2 == 'user':\n",
    "                        move = tuple(int(x.strip()) for x in input(\"\\nInput your move: \").split(' '))\n",
    "                        print(\"\\n\")\n",
    "                        action = game.move_to_int(move)\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "                    else:\n",
    "                        action = mcts.search([state], player)\n",
    "                        action = np.argmax(action)\n",
    "                        print(f\"\\nAlphaZero Action: {game.int_to_move(action)}\\n\")\n",
    "                        state = game.get_next_state(state, action, player)\n",
    "\n",
    "                winner, win = game.get_value_and_terminated(state, action, player)\n",
    "                if win:\n",
    "                    game.print_board(state)\n",
    "                    print(f\"player {winner} wins\")\n",
    "                    exit()\n",
    "                    break\n",
    "\n",
    "                player = -player\n",
    "                game.print_board(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
